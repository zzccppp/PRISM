@misc{zheng2025graphvqtransformergvtfast,
      title={Graph VQ-Transformer (GVT): Fast and Accurate Molecular Generation via High-Fidelity Discrete Latents}, 
      author={Haozhuo Zheng and Cheng Wang and Yang Liu},
      year={2025},
      eprint={2512.02667},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2512.02667}, 
      code={https://github.com/zzccppp/GVT},
      abstract={The de novo generation of molecules with desirable properties is a critical challenge, where diffusion models are computationally intensive and autoregressive models struggle with error propagation. In this work, we introduce the Graph VQ-Transformer (GVT), a two-stage generative framework that achieves both high accuracy and efficiency. The core of our approach is a novel Graph Vector Quantized Variational Autoencoder (VQ-VAE) that compresses molecular graphs into high-fidelity discrete latent sequences. By synergistically combining a Graph Transformer with canonical Reverse Cuthill-McKee (RCM) node ordering and Rotary Positional Embeddings (RoPE), our VQ-VAE achieves near-perfect reconstruction rates. An autoregressive Transformer is then trained on these discrete latents, effectively converting graph generation into a well-structured sequence modeling problem. Crucially, this mapping of complex graphs to high-fidelity discrete sequences bridges molecular design with the powerful paradigm of large-scale sequence modeling, unlocking potential synergies with Large Language Models (LLMs). Extensive experiments show that GVT achieves state-of-the-art or highly competitive performance across major benchmarks like ZINC250k, MOSES, and GuacaMol, and notably outperforms leading diffusion models on key distribution similarity metrics such as FCD and KL Divergence. With its superior performance, efficiency, and architectural novelty, GVT not only presents a compelling alternative to diffusion models but also establishes a strong new baseline for the field, paving the way for future research in discrete latent-space molecular generation.}
}

@inproceedings{xu2024manifoundation,
  title={Manifoundation model for general-purpose robotic manipulation of contact synthesis with arbitrary objects and robots},
  author={Xu, Zhixuan and Gao, Chongkai and Liu, Zixuan and Yang, Gang and Tie, Chenrui and Zheng, Haozhuo and Zhou, Haoyu and Peng, Weikun and Wang, Debang and Hu, Tianrun and others},
  booktitle={2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  pages={10905--10912},
  year={2024},
  organization={IEEE}
}
